# Company Policy Advisor

An AI-powered assistant that enables employees to quickly query internal business documents like HR guidelines, operational policies, and compliance manuals â€” running fully locally without reliance on external APIs or internet access.

---

## Business Problem

In many companies, critical policies are locked inside large documents which employees rarely read fully.

Common problems faced:
- Wasted time searching for information manually.
- Incorrect or outdated advice from staff or managers.
- Increased compliance risk due to misinterpretation of legal policies.
- Overwhelmed HR departments answering repetitive queries.

**Our project solves these by enabling instant, accurate answers based solely on internal documentation, ensuring consistency and improving operational efficiency.**

---

## Solution Overview

| Category | Details |
|:---|:---|
| **Frontend** | Streamlit |
| **Backend** | Python 3.10+, LangChain |
| **LLM Model** | Mistral 7B Instruct (v0.1 Q4_K_S) running locally with Ollama |
| **Embedding Model** | SentenceTransformers `all-MiniLM-L6-v2` |
| **Vector Database** | FAISS (v1.8.0, CPU version) |
| **Local LLM Runtime** | Ollama (v0.6.6) |
| **Repository** | [Company Policy Advisor GitHub](https://github.com/MohamedMaknojiya/company-policy-advisor) |

All components run 100% on the user's machine for maximum privacy and speed.

---

## Problems Faced During Development

### 1. Chunk Duplication
Certain business documents repeated similar content in multiple sections.  
Simple retrieval led to redundant information appearing in answers.

**Solution:**  
Retrieve multiple relevant chunks, merge them, and summarise using the LLM to remove duplicates.

---

### 2. Model Response Speed
Initial direct loading of large models inside Python caused slow first-token generation.  
HuggingFace transformers were too heavy for efficient local inference.

**Solution:**  
Switched to using Ollama for serving the Mistral model externally, drastically improving response time.

---

### 3. LangChain Deprecation Warnings
Midway through the project, LangChain library updated its module import structure.

**Solution:**  
Manually updated critical imports and adjusted embedding loading as per new standards.

---

## Document Preprocessing Pipeline

1. **Extract text** from uploaded PDF using `PyMuPDF` (`fitz`).
2. **Chunk** the document into ~500 character blocks with ~50 token overlap.
3. **Embed** each chunk using `all-MiniLM-L6-v2`.
4. **Store** vectors inside FAISS.
5. **Retrieve** top 5 relevant chunks for user queries.
6. **Merge and summarise** results to generate a clean, final answer.

---

## Deployment Instructions

### 1. Clone the Repository
```bash
git clone git@github.com:MohamedMaknojiya/company-policy-advisor.git
cd company-policy-advisor
```

### 2. Set Up Python Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Python Dependencies
```bash
pip install -r requirements.txt
```

### 4. Install and Start Ollama
```bash
brew install ollama
ollama serve
ollama pull mistral
```

### 5. Run the Application
```bash
streamlit run main.py
```

Visit [http://localhost:8501](http://localhost:8501) after running.

---

## Uploading Business Documents

- Open the Streamlit app.
- Upload your internal business PDF documents.
- The documents will be automatically processed, indexed, and made ready for question-answering.

**Supported formats:**  
- PDF only (more coming soon)

---

## Note

This repository currently uses `.gitignore` to exclude:
- Local model files
- Large vectorstores
- Cache files
- Virtual environments

This keeps the GitHub repository lightweight and focused only on essential source code.

---
